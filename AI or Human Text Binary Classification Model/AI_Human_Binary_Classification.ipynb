{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(bookReviewDataSet_filename, header=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I decided to use \"AI vs Human Text\" by SHAYAN GERAMI on Kaggle.\n",
    "2. It will predict which text is generated by AI or not\n",
    "3. This is a supervised learning binary classification problem\n",
    "4. The features are text data in a column, and the label would be if it's generated or not (0 or 1)\n",
    "5. Many people have been using ChatGPT or other AI models for help, but sometimes they use it to complete assignments fully of it which is a breach of academic policies. This goes further than school, and professional careers such as research literature are noticing the effects of AI written text.\n",
    "---- Extra. I wanted to initially use this but dataset file was too large and didn't particulary know how to understand the data.\n",
    "1. I decided to use the dataset \"AI-ArtBench\" by Ravidu Silva and Jordan J. Bird from Kaggle. I chose this one because it's frequently updated and has 600 downloads.\n",
    "2. I want to be able to predict given a piece of art whether or not it is AI\n",
    "3. This would be a supervised classification problem. It will be binary classification, though it's interesting to do multi-class (maybe they drew it and used AI for help, or even traced something!) in the future.\n",
    "4. Features are different art genres i.e. impressionalism, renaissance\n",
    "5. It is extremely common in recent times for people to use AI for art. Many times the art used to make said AI art is from actual artists who do this for a living. Being able to identify which is AI ensures no one is being scammed (it's common for AI artists to claim they made the art by hand and make customers pay hundreds of dollars), and in the future identify possible artists that the AI took from, to alert them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanvsAItext_filename = os.path.join(\"data\", \"AI_Human.csv\") \n",
    "get_data = pd.read_csv(humanvsAItext_filename, header=0, nrows=38152)\n",
    "df = get_data.sample(n=10000, random_state=1234) #Cropping data to randomized 10000 - smaller data set for memory purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "['text', 'generated']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18105</th>\n",
       "      <td>A Cowboy Who Rode the Waves\\n\\nIn the wild, va...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4909</th>\n",
       "      <td>The use of Facial Action Coding System to read...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22784</th>\n",
       "      <td>The Electoral College system, which is the pro...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22770</th>\n",
       "      <td>The electoral college is a system in the Unite...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26873</th>\n",
       "      <td>His adventures sounded exciting, but I was cur...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>car usage should be limited across the world t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>The new technology called Facial Action Coding...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9587</th>\n",
       "      <td>\\nOnline schooling has become an increasingly ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22167</th>\n",
       "      <td>Car-free cities: An Eco-Friendly Future?\\n\\nIm...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>The study of ocean acidification has become in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated\n",
       "18105  A Cowboy Who Rode the Waves\\n\\nIn the wild, va...        1.0\n",
       "4909   The use of Facial Action Coding System to read...        0.0\n",
       "22784  The Electoral College system, which is the pro...        1.0\n",
       "22770  The electoral college is a system in the Unite...        1.0\n",
       "26873  His adventures sounded exciting, but I was cur...        1.0\n",
       "2495   car usage should be limited across the world t...        0.0\n",
       "3936   The new technology called Facial Action Coding...        0.0\n",
       "9587   \\nOnline schooling has become an increasingly ...        1.0\n",
       "22167  Car-free cities: An Eco-Friendly Future?\\n\\nIm...        1.0\n",
       "12342  The study of ocean acidification has become in...        1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(list(df.columns))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI generated texts: 5533\n",
      "Human written texts : 4467\n"
     ]
    }
   ],
   "source": [
    "#0 means human written, 1 means AI generated\n",
    "ai_generated_count = (df['generated'] == 1.0).sum()\n",
    "human_generated_count = (df['generated'] == 0.0).sum()\n",
    "print(\"AI generated texts:\", ai_generated_count)\n",
    "print(\"Human written texts :\", human_generated_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          generated\n",
       "count  10000.000000\n",
       "mean       0.553300\n",
       "std        0.497176\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data? \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Label x and y and do train_test_split()\n",
    "2. Tranform text using vectorizer\n",
    "3. Build neural network with Keras OR use logistic regression (maybe neural network so we can find more complex patterns in the generated text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 12:13:15.056366: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-03 12:13:15.056398: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['generated'].astype(bool)\n",
    "X = df['text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32711\n"
     ]
    }
   ],
   "source": [
    "#Transforming text - TDF vector\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "vocab_size = len(tfidf_vectorizer.vocabulary_)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 32711)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2093568   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,096,193\n",
      "Trainable params: 2,096,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 12:13:22.471474: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-08-03 12:13:22.471505: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-08-03 12:13:22.471581: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (i-095f281bde303de91): /proc/driver/nvidia/version does not exist\n",
      "2024-08-03 12:13:22.471823: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "nn_model = keras.Sequential()\n",
    "input_layer = keras.layers.InputLayer(input_shape=(vocab_size,))\n",
    "nn_model.add(input_layer)\n",
    "\n",
    "nn_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "hidden_layer_1 = keras.layers.Dense(units=64, activation='relu')\n",
    "nn_model.add(hidden_layer_1)\n",
    "\n",
    "nn_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "hidden_layer_2 = keras.layers.Dense(units=32, activation='relu')\n",
    "nn_model.add(hidden_layer_2)\n",
    "\n",
    "nn_model.add(keras.layers.Dropout(.25))\n",
    "\n",
    "hidden_layer_3 = keras.layers.Dense(units=16, activation='relu')\n",
    "nn_model.add(hidden_layer_3)\n",
    "\n",
    "\n",
    "output_layer = keras.layers.Dense(units=1, activation='sigmoid')\n",
    "nn_model.add(output_layer)\n",
    "\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = keras.optimizers.SGD(learning_rate=0.1)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "nn_model.compile(optimizer=sgd_optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced from previous assignments\n",
    "class ProgBarLoggerNEpochs(keras.callbacks.Callback):\n",
    "    def __init__(self, num_epochs: int, every_n: int = 50):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.every_n = every_n\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every_n == 0:\n",
    "            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)\n",
    "            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v) for k, v in logs.items()]\n",
    "            s_list = [s] + logs_s\n",
    "            print(', '.join(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-03 12:13:24.942459: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-08-03 12:13:24.947188: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2649995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 35], Loss: 0.0702, Accuracy: 0.9763, Val_loss: 0.0554, Val_accuracy: 0.9833\n",
      "Epoch [10/ 35], Loss: 0.0296, Accuracy: 0.9905, Val_loss: 0.0440, Val_accuracy: 0.9853\n",
      "Epoch [15/ 35], Loss: 0.0247, Accuracy: 0.9928, Val_loss: 0.0390, Val_accuracy: 0.9853\n",
      "Epoch [20/ 35], Loss: 0.0103, Accuracy: 0.9972, Val_loss: 0.0391, Val_accuracy: 0.9867\n",
      "Epoch [25/ 35], Loss: 0.0633, Accuracy: 0.9845, Val_loss: 0.0424, Val_accuracy: 0.9853\n",
      "Epoch [30/ 35], Loss: 0.0090, Accuracy: 0.9970, Val_loss: 0.0474, Val_accuracy: 0.9867\n",
      "Epoch [35/ 35], Loss: 0.0066, Accuracy: 0.9975, Val_loss: 0.0407, Val_accuracy: 0.9873\n",
      "Elapsed time: 35.98s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_epochs = 35\n",
    "history = nn_model.fit(X_train_tfidf.toarray(), y_train, epochs=num_epochs, verbose=0, validation_split=0.2, callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)])\n",
    "end_time = time.time()\n",
    "print('Elapsed time: %.2fs' % (end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial version of NN output (num_epochs = 55):\\\n",
    "Epoch [5/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0664, Val_accuracy: 0.9867 \\\n",
    "Epoch [10/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0669, Val_accuracy: 0.9867\\\n",
    "Epoch [15/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0675, Val_accuracy: 0.9867\\\n",
    "Epoch [20/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0680, Val_accuracy: 0.9867\\\n",
    "Epoch [25/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0684, Val_accuracy: 0.9867\\\n",
    "Epoch [30/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0689, Val_accuracy: 0.9867\\\n",
    "Epoch [35/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0692, Val_accuracy: 0.9867\\\n",
    "Epoch [40/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0697, Val_accuracy: 0.9867\\\n",
    "Epoch [45/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0700, Val_accuracy: 0.9867\\\n",
    "Epoch [50/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0703, Val_accuracy: 0.9867\\\n",
    "Epoch [55/ 55], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0706, Val_accuracy: 0.9867\\\n",
    "Elapsed time: 32.04s\\\n",
    "Output appears to have now val lost but high accuracy, which is a good sign. It might imply overfitting since there is no Loss in general.\n",
    "Second edition (num_epochs = 35)\\\n",
    "Epoch [5/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0710, Val_accuracy: 0.9867\\\n",
    "Epoch [10/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0712, Val_accuracy: 0.9867\\\n",
    "Epoch [15/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0715, Val_accuracy: 0.9867\\\n",
    "Epoch [20/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0718, Val_accuracy: 0.9867\\\n",
    "Epoch [25/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0721, Val_accuracy: 0.9867\\\n",
    "Epoch [30/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0723, Val_accuracy: 0.9867\\\n",
    "Epoch [35/ 35], Loss: 0.0000, Accuracy: 1.0000, Val_loss: 0.0725, Val_accuracy: 0.9867\\\n",
    "Elapsed time: 21.83s\\\n",
    "More val_loss, but predicting the same amount with val_accuracy - will also add a drop out layers in between all hidden layers\n",
    "Third Edition (Added dropout layers, got rid of 4th hidden layer): \\\n",
    "Epoch [5/ 35], Loss: 0.0749, Accuracy: 0.9798, Val_loss: 0.0821, Val_accuracy: 0.9727\\\n",
    "Epoch [10/ 35], Loss: 0.0252, Accuracy: 0.9938, Val_loss: 0.0775, Val_accuracy: 0.9760\\\n",
    "Epoch [15/ 35], Loss: 0.0285, Accuracy: 0.9930, Val_loss: 0.0514, Val_accuracy: 0.9867\\\n",
    "Epoch [20/ 35], Loss: 0.0246, Accuracy: 0.9930, Val_loss: 0.0692, Val_accuracy: 0.9807\\\n",
    "Epoch [25/ 35], Loss: 0.1042, Accuracy: 0.9693, Val_loss: 0.0584, Val_accuracy: 0.9813\\\n",
    "Epoch [30/ 35], Loss: 0.0096, Accuracy: 0.9977, Val_loss: 0.0663, Val_accuracy: 0.9840\\\n",
    "Epoch [35/ 35], Loss: 0.0063, Accuracy: 0.9985, Val_loss: 0.0730, Val_accuracy: 0.9853\\\n",
    "Elapsed time: 23.13s \\\n",
    "This one to me makes more sense for the data. Having no Loss was a big concerning. Will now move one of the drop out layers after input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9888\n",
      "Loss:  0.03994454815983772 Accuracy:  0.9887999892234802\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = nn_model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn_model.predict(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology is becoming a bigger part of our lives. As time goes on, we develop new technology for different purposes. One such purpose is to read emotions. This technology will benefit our society in different ways, like in education. The technology to read students emotions is valuable to our schools.\n",
      "\n",
      "The first way in which this technology is vaulable to schools is by improving lessons. Often times, many students of different calibers are thrust into the same classroom. While some grow bored others become confused by the content of the lesson. In the article, Dr. Huang is quoted saying, \"A classroom computer could recognize when a student is becoming confused or bored,\" and then \"Then it could modify the lesson, like an effective human instructor. If we implemented this emotion recognition technology, we could help students of all levels grow and reach their potential.\n",
      "\n",
      "Another reason why technology that can recognize emotions is vaulable in schools is because of fighting and bullying. In paragraphs three and four of the article, it discusses how the technology works and what it can do. It claims that it can dectect mixed emotions and even, in paragraph 8, detect fake smiling. This technology could be used to detect when students are angry and wanting to fight or even sad from being bullied. Teachers cannot always detect these things from students, especially if they are trying to hide them.\n",
      "\n",
      "In addition to stopping fights and helping bullied students, this emotion recognition technology could assist in identifying students who have problematic home lives and need help. Although this would not always be 100% accurate, it could help faculty see those students who might be depressed or angry, and need help. This would aid in making schools a happier place. It could also negate problems that they could have later in life. Installing this technology in our schools would be highly beneficial.\n",
      "\n",
      "Schools need the technology to read students emotions. It would fit in with other technological advancements that have improved our education system. Advances in technology helped to improve not just our education, but our overall lives. Numerous people in our world cannot live without technology. Technology is important to our society.          \n",
      "\n",
      "Prediction: Is this a AI generated? False\n",
      "\n",
      "Actual: Is this AI generated? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_test.to_numpy()[11])\n",
    "isAI = True if predictions[11] >= .5 else False\n",
    "print('\\nPrediction: Is this a AI generated? {}\\n'.format(isAI))\n",
    "print('Actual: Is this AI generated? {}\\n'.format(y_test.to_numpy()[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
